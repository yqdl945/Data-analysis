# 数据分析45讲



## 05|Pandas

- Series
- DataFrame

 ~~~
import pandas as pd
from pandas import DataFrame,Series
# 数据清洗
## 1.删除不必要值——drop(放入要删除的目标参数)
df1 = df1.drop(columns = ['XXX'])
df2 = df1.drop(index = ['YYY'])
## 2.重命名——rename("错误名称":"正确名称")
df2.rename(columns={'Chinese': 'YuWen', 'English': 'Yingyu'}, inplace = True)
## 3.去除重复值——drop_duplicates()
df = df.drop_duplicates() 
## 4.更改格式
### 4.1.astype('str')
df2['Chinese'].astype('str') 
df2['Chinese'].astype(np.int64) 
### 4.2 srip("也可以添加符号用于删除数据中的特殊符号") #默认删除左右空格
df2['Chinese']=df2['Chinese'].map(str.strip())
df2['Chinese']=df2['Chinese'].str.strip('$')

## 找空值——isnull()
 ~~~

### apply()函数清洗
### describe()输出结果！
### lambda

~~~
## merget(a,b,on = "基于目标")
## merget(a,b,how = "inner")
## left/right
## outer
~~~
### 基于SQL打开——pandasql
~~~
import pandas as pd
from pandas import DataFrame
from pandasql import sqldf, load_meat, load_births
df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
pysqldf = lambda sql: sqldf(sql, globals())
sql = "select * from df1 where name ='ZhangFei'"
print pysqldf(sql)

~~~

## 15|数据可视化

- 视图分类：
  - 比较
  - 联系
  - 构成
  - 分布
- 视图：![img](https://static001.geekbang.org/resource/image/46/75/4673a17085302cfe9177f8ee687ac675.png)

- matplotlib使用：

  ~~~python
  import matplotlib.pyplot as plt
  #散点图：
  plt.scatter(x,y,marker = None)
  #marker是点的样子？
  # 绘图步骤
  x = 
  y = 
  plt.scatter(x,y,marker = '?')
  plt.show(不填参数！！)
  ~~~

  - 默认形状是长方形

- seaborn

  ~~~python
  import seaborn as sns
  # 绘图:
  df = DataFrame(,)
  sns.joinplot(x = 'x',y = 'y',data = df,kind = 'scatter')
  plt.show()
  # kind 作用？
  ~~~

  - 默认形状是正方形 + 变量分布情况

  

  ........

  ![img](https://static001.geekbang.org/resource/image/8e/d2/8ed2addb00a4329dd63bba669f427fd2.png)

  

## 二 算法

### | 决策树  上

- 将经验总结出来，根据因素决定结果

  - 构造：根节点；内部节点；叶节点（停止，没有子节点）——（父子关系）
  - 剪枝：预剪枝（构造时剪枝）；后剪枝（生成决策树之后进行剪枝）
    - 防止“过拟合”——模型训练的过于完美，在实际应用中存在“死板”现象造成分类错误
      - 训练集中，样本量小——模型泛化能力差

- 选择哪个属性作为根节点；选择那些属性作为后继节点；什么时候停止并得到目标值

  - 纯度（目标变量分歧最小）；信息熵(信息的不确定度)

  - 信息熵数学公式

  - 熵越大，纯度越低

    - 经典“不纯度”指标：信息增益（ID3）、信息增益率（c4.5）、基尼指数（Cart算法）

      - 信息增益(ID3)：缺点：	倾向于选择取值多的属性，（有些属性**对分类无帮助**，可能会被选为**最优属性**
        -  改进：信息增益率(C4.5) 信息增益/属性熵
        - 悲观剪枝(PEP)--不需要单独测试数据集
        - 离散化处理连续属性--选择具有最高信息增益的划分对应的阈值
        - 处理缺失值--

    - 综合：ID3 方法简单，-对噪声敏感

      ​			C4.5需要对数据集进行多次扫描，效率较低！

      

### |决策树  中

- CART算法(Classification And Regression Tree) :分类回归树

  - 只支持二叉树
  - 属性选择指标基于基尼系数——选择基尼系数是最小的属性
  - 可以采用最小绝对偏差/最小二乘偏差作为划分节点的依据

  分类树：基于数据判断、选择

  - 可以处理离散数据

  回归树：给定数据——预测

  - 基于偏差做判断

  ~~~python
  # 创建方法：sklearn
  model_selection - train_test_split
  metrics - accuracy_score
  tree - DecisionTreeClassifier
  datasets - loda_iris
  
  ~~~

- 评价指标：

  ​	样本的混轮程度（样本离散程度）:方差

  ~~~python
  
  # encoding=utf-8
  from sklearn.metrics import mean_squared_error
  from sklearn.model_selection import train_test_split
  from sklearn.datasets import load_boston
  from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error
  from sklearn.tree import DecisionTreeRegressor
  # 准备数据集
  boston=load_boston()
  # 探索数据
  print(boston.feature_names)
  # 获取特征集和房价
  features = boston.data
  prices = boston.target
  # 随机抽取33%的数据作为测试集，其余为训练集
  train_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=0.33)
  # 创建CART回归树
  dtr=DecisionTreeRegressor()
  # 拟合构造CART回归树
  dtr.fit(train_features, train_price)
  # 预测测试集中的房价
  predict_price = dtr.predict(test_features)
  # 测试集的结果评价
  print('回归树二乘偏差均值:', mean_squared_error(test_price, predict_price))
  print('回归树绝对值偏差均值:', mean_absolute_error(test_price, predict_price)) 
  ~~~

- 剪枝！

  - CCP（cost-complexity prune）——后剪枝；代价复杂度
    - 表面误差增益值 

  





### |决策树 下

 决策树实战：TITANIC

参数表：正常使用默认参数！

预测：训练集；测试数据集

流程：数据探索-清洗-**特征值**

决策树模型-评估&预测-可视化



- 数据探索：

  ~~~python
  #数据预览
  data = pd.read_csv('xxx')
  print(data.indo())
  print(data.describe())
  print(data.describe(info = 'O'))
  print(data.head())
  print(data.tail())
  ~~~

  

- 数据清洗：

  - 根据数据缺失，查看数据类型
    - 数值型通过 **平均数**来补齐
    - 非数值型的，通过相关联的比例（占比最大的进行补齐）

- 特征选择：

  - 抛弃
    - 抛弃对分类没有作用的；
    - 缺失值太多的也放弃
    - 杂乱无章；无规律的
  - 剩余的放入特征向量features里面
    - 将字符串使用数字代表！（类）
      - Dictvectorizer
  - 使用算法 fit 进行训练；将特征值矩阵，分类表直接过作为参数传入——决策树分类器

  - 模型评估：

    - K折交叉验证：做K次交叉实验，每次取1/K数据验证，其余作为训练，轮流K次，AVG（多样本-取平均值-进行样本估计）（放回抽样）建议（K = 10) 

      验证模型准确率

  - 决策树可视化

    - 利用Graphviz帮助**决策树可视化**

- 总结：

  - 特征选择——（关系）分类模型的好坏——特征值矩阵
  - 模型准确率需要考虑与测试集的实际结果对比。，无测试集结果，使用K折交叉验证(cross_val_score)
  - 